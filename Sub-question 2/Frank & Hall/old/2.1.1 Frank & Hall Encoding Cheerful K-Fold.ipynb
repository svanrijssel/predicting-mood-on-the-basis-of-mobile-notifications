{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_validate, KFold\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, cohen_kappa_score, mean_absolute_error, f1_score\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, GaussianNoise\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/Stijn/Documents/Master Data Science and Society/Block 3/thesis/code/thesis_Code/\"\n",
    "mood = pd.read_csv(path+'mood_imputed_median.csv', sep = ',', index_col=0)\n",
    "mood['cheerful_last'] = mood.groupby('user_id')['cheerful'].shift()\n",
    "mood.loc[(pd.isnull(mood.cheerful_last)), 'cheerful_last'] = mood['cheerful']\n",
    "mood_cheerful = mood.drop([\"content\", \"bored\", \"anxious\", \"user_id\", \"response_time\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize predictors\n",
    "mood_normz = mood_cheerful.copy()\n",
    "features_to_normalize = ['day_time_window', 'average_TimeUse', 'messaging', 'socialnetworking', 'otherapp']\n",
    "mood_normz[features_to_normalize] = mood_normz[features_to_normalize].apply(lambda x:(x-x.min()) / (x.max()-x.min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = predictors, y = target\n",
    "X = mood_normz.iloc[:,1:]\n",
    "y = mood_normz.iloc[:, 0:1]\n",
    "\n",
    "# Convert float to int\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before OverSampling, counts of labels: 3    3719\n",
      "2    2966\n",
      "4    2358\n",
      "1    1880\n",
      "0    1413\n",
      "5     885\n",
      "Name: cheerful, dtype: int64\n",
      "After OverSampling, counts of label '0': 3719\n",
      "After OverSampling, counts of label '1': 3719\n",
      "After OverSampling, counts of label '2': 3719\n",
      "After OverSampling, counts of label '3': 3719\n",
      "After OverSampling, counts of label '4': 3719\n",
      "After OverSampling, counts of label '5': 3719\n"
     ]
    }
   ],
   "source": [
    "# Oversampling unbalanced target\n",
    "print(\"Before OverSampling, counts of labels: {}\".format(y['cheerful'].value_counts()))\n",
    "  \n",
    "sm = SMOTENC(categorical_features=[0, 2, 6], random_state = 2) \n",
    "X_train, y_train = sm.fit_sample(X, y['cheerful'].ravel()) \n",
    "\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train == 0))) \n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train == 1))) \n",
    "print(\"After OverSampling, counts of label '2': {}\".format(sum(y_train == 2))) \n",
    "print(\"After OverSampling, counts of label '3': {}\".format(sum(y_train == 3))) \n",
    "print(\"After OverSampling, counts of label '4': {}\".format(sum(y_train == 4))) \n",
    "print(\"After OverSampling, counts of label '5': {}\".format(sum(y_train == 5))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert oversample dataset to Pandas DataFrame\n",
    "X = pd.DataFrame(data=X_train)\n",
    "y = pd.DataFrame(data=y_train, columns = ['cheerful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cheerful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cheerful\n",
       "0         3\n",
       "1         3\n",
       "2         3\n",
       "3         3\n",
       "4         3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frank & hall encoding\n",
    "\n",
    "def multiple_appends(listname, *element):\n",
    "    listname.extend(element)\n",
    "\n",
    "y_trainFH = []\n",
    "for index, row in y.iterrows():\n",
    "    frankhalltrain = []\n",
    "    if row['cheerful'] == 0:\n",
    "        multiple_appends(frankhalltrain, 0, 0, 0, 0, 0)\n",
    "    elif row['cheerful'] == 1:\n",
    "        multiple_appends(frankhalltrain, 1, 0, 0, 0, 0)\n",
    "    elif row['cheerful'] == 2:\n",
    "        multiple_appends(frankhalltrain, 1, 1, 0, 0, 0)\n",
    "    elif row['cheerful'] == 3:\n",
    "        multiple_appends(frankhalltrain, 1, 1, 1, 0, 0)\n",
    "    elif row['cheerful'] == 4:\n",
    "        multiple_appends(frankhalltrain, 1, 1, 1, 1, 0)\n",
    "    elif row['cheerful'] == 5:\n",
    "        multiple_appends(frankhalltrain, 1, 1, 1, 1, 1)\n",
    "    y_trainFH.append(frankhalltrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataset to Pandas DataFrame\n",
    "y = pd.DataFrame(data=y_trainFH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.iloc[:,:-1]\n",
    "X_onlylastmood = X.iloc[:,-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macroaverage_mae(test, pred):\n",
    "    mae_0, mae_1, mae_2, mae_3, mae_4, mae_5 = (0,0,0,0,0,0)\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        if test[i] == 0 and pred[i] != 0:\n",
    "            mae_0 += (abs(test[i]-pred[i]))\n",
    "        if test[i] == 1 and pred[i] != 1:\n",
    "            mae_1 += (abs(test[i]-pred[i]))\n",
    "        if test[i] == 2 and pred[i] != 2:\n",
    "            mae_2 += (abs(test[i]-pred[i]))\n",
    "        if test[i] == 3 and pred[i] != 3:\n",
    "            mae_3 += (abs(test[i]-pred[i]))\n",
    "        if test[i] == 4 and pred[i] != 4:\n",
    "            mae_4 += (abs(test[i]-pred[i]))\n",
    "        if test[i] == 5 and pred[i] != 5:\n",
    "            mae_5 += (abs(test[i]-pred[i]))\n",
    "\n",
    "    cnt_0, cnt_1, cnt_2, cnt_3, cnt_4, cnt_5 = (0,0,0,0,0,0)\n",
    "    for z in range(len(test)):\n",
    "        if test[z] == 0:\n",
    "            cnt_0 += 1\n",
    "        if test[z] == 1:\n",
    "            cnt_1 += 1\n",
    "        if test[z] == 2:\n",
    "            cnt_2 += 1\n",
    "        if test[z] == 3:\n",
    "            cnt_3 += 1\n",
    "        if test[z] == 4:\n",
    "            cnt_4 += 1\n",
    "        if test[z] == 5:\n",
    "            cnt_5 += 1\n",
    "\n",
    "    mae_macroaverage = ((mae_0/cnt_0) + (mae_1/cnt_1) + (mae_2/cnt_2) + (mae_3/cnt_3) + (mae_4/cnt_4) + (mae_5/cnt_5)) / 6\n",
    "    return mae_macroaverage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fh_pred(pred):\n",
    "    FH_pred = []\n",
    "    for i in pred:\n",
    "        temp_list = []\n",
    "        for binary in i:\n",
    "            if binary >= 0.5:\n",
    "                temp_list.append(1)\n",
    "            else:\n",
    "                temp_list.append(0)\n",
    "        FH_pred.append(temp_list)\n",
    "\n",
    "    FH_prediction = pd.DataFrame(data=FH_pred, columns=['mood_0', 'mood_1', 'mood_2', 'mood_3', 'mood_4'])\n",
    "    real_pred = FH_prediction.sum(axis=1)\n",
    "    return real_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fh_test(test):\n",
    "    real_test = test.sum(axis=1)\n",
    "    return real_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Stijn/Library/Python/3.7/lib/python/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "n_split=10\n",
    "f1_scores = []\n",
    "mmae_scores = []\n",
    "for train_index, test_index in KFold(n_splits = n_split, random_state=2, shuffle=True).split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=6, activation='relu'))\n",
    "    model.add(Dense(12, activation='relu'))\n",
    "    model.add(Dense(5, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train,epochs=100, batch_size=512, verbose=0)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    pred = transform_fh_pred(y_pred).values\n",
    "    test = transform_fh_test(y_test).values\n",
    "    \n",
    "    f1_scores.append(round(f1_score(test, pred, average='weighted'), 4))\n",
    "    mmae_scores.append(macroaverage_mae(test, pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.093, 0.0986, 0.0947, 0.0915, 0.0966, 0.0951, 0.0814, 0.1006, 0.0855, 0.0967]\n",
      "[1.4769, 1.4593, 1.4513, 1.4661, 1.4371, 1.4744, 1.4486, 1.463, 1.4538, 1.4364]\n"
     ]
    }
   ],
   "source": [
    "print(f1_scores)\n",
    "\n",
    "mmae_scores_rounded = [ round(elem, 4) for elem in mmae_scores ]\n",
    "print(mmae_scores_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 11.493383742911153\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score\n",
    "a = accuracy_score(pred,test)\n",
    "print('Accuracy is:', a*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Same Dif\n",
      "correct 193 111\n",
      "wrong   1588 753\n",
      "Table is right!\n"
     ]
    }
   ],
   "source": [
    "# true = y_test\n",
    "# pred = y_pred\n",
    "# last_value = X_bored_lastarray\n",
    "same_correct = 0\n",
    "same_wrong = 0\n",
    "dif_correct = 0\n",
    "dif_wrong = 0\n",
    "\n",
    "X_test_bored = X_test_onlylastmood['bored_last'].values\n",
    "for i in range(len(pred)):\n",
    "    if (test[i] == X_test_bored[i]) and (test[i] == pred[i]):\n",
    "        same_correct += 1\n",
    "    if (test[i] == X_test_bored[i]) and (test[i] != pred[i]):\n",
    "        same_wrong += 1\n",
    "    if (test[i] != X_test_bored[i]) and (test[i] == pred[i]):\n",
    "        dif_correct += 1\n",
    "    if (test[i] != X_test_bored[i]) and (test[i] != pred[i]):\n",
    "        dif_wrong += 1   \n",
    "\n",
    "print(\"       \", \"Same\", \"Dif\")\n",
    "print(\"correct\", same_correct, dif_correct)\n",
    "print(\"wrong  \", same_wrong, dif_wrong)\n",
    "\n",
    "#Check if table is similar to accuracy\n",
    "if (same_correct+dif_correct) / (same_correct+dif_correct+same_wrong+dif_wrong) == a:\n",
    "    print(\"Table is right!\")\n",
    "else:\n",
    "    print(\"Table is wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted valxues:  Counter({5: 1368, 0: 405, 3: 344, 4: 319, 1: 116, 2: 93})\n",
      "True values:  Counter({0: 1264, 1: 491, 2: 455, 3: 301, 4: 119, 5: 15})\n",
      "Previous mood: Counter({0.0: 1286, 1.0: 494, 2.0: 451, 3.0: 294, 4.0: 106, 5.0: 14})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(\"Predicted valxues: \", Counter(pred))\n",
    "print(\"True values: \", Counter(test))\n",
    "print(\"Previous mood:\", Counter(X_test_bored))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length:  2645\n",
      "Same Correct 0 guesses is:  151\n",
      "Same Wrong 0 guesses is:  888\n",
      "Dif Correct 0 guesses is:  53\n",
      "Dif Wrong 0 guesses is:  172\n",
      "\n",
      "Same Correct 1 guesses is:  10\n",
      "Same Wrong 1 guesses is:  262\n",
      "Dif Correct 1 guesses is:  14\n",
      "Dif Wrong 1 guesses is:  205\n",
      "\n",
      "Same Correct 2 guesses is:  3\n",
      "Same Wrong 2 guesses is:  229\n",
      "Dif Correct 2 guesses is:  12\n",
      "Dif Wrong 2 guesses is:  211\n",
      "\n",
      "Same Correct 3 guesses is:  27\n",
      "Same Wrong 3 guesses is:  156\n",
      "Dif Correct 3 guesses is:  11\n",
      "Dif Wrong 3 guesses is:  107\n",
      "\n",
      "Same Correct 4 guesses is:  2\n",
      "Same Wrong 4 guesses is:  53\n",
      "Dif Correct 4 guesses is:  12\n",
      "Dif Wrong 4 guesses is:  52\n",
      "\n",
      "Same Correct 5 guesses is:  0\n",
      "Same Wrong 5 guesses is:  0\n",
      "Dif Correct 5 guesses is:  9\n",
      "Dif Wrong 5 guesses is:  6\n"
     ]
    }
   ],
   "source": [
    "same_correct_0, same_wrong_0, same_correct_1, same_wrong_1, same_correct_2, same_wrong_2, same_correct_3, same_wrong_3, same_correct_4, same_wrong_4, same_correct_5, same_wrong_5 = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "dif_correct_0, dif_wrong_0, dif_correct_1, dif_wrong_1, dif_correct_2, dif_wrong_2, dif_correct_3, dif_wrong_3, dif_correct_4, dif_wrong_4, dif_correct_5, dif_wrong_5 = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    if test[i] == 0 and pred[i] == 0:\n",
    "        if test[i] == X_test_bored[i]:\n",
    "            same_correct_0 += 1\n",
    "        if test[i] != X_test_bored[i]:\n",
    "            dif_correct_0 += 1\n",
    "    if test[i] == 0 and pred[i] != 0:\n",
    "        if test[i] == X_test_bored[i]:\n",
    "            same_wrong_0 += 1\n",
    "        if test[i] != X_test_bored[i]:\n",
    "            dif_wrong_0 += 1\n",
    "\n",
    "    if test[i] == 1 and pred[i] == 1:\n",
    "        if test[i] == X_test_bored[i]:\n",
    "            same_correct_1 += 1\n",
    "        if test[i] != X_test_bored[i]:\n",
    "            dif_correct_1 += 1\n",
    "    if test[i] == 1 and pred[i] != 1:\n",
    "        if test[i] == X_test_bored[i]:\n",
    "            same_wrong_1 += 1\n",
    "        if test[i] != X_test_bored[i]:\n",
    "            dif_wrong_1 += 1\n",
    "        \n",
    "    if test[i] == 2 and pred[i] == 2:\n",
    "        if test[i] == X_test_bored[i]:\n",
    "            same_correct_2 += 1\n",
    "        if test[i] != X_test_bored[i]:\n",
    "            dif_correct_2 += 1\n",
    "    if test[i] == 2 and pred[i] != 2:\n",
    "        if test[i] == X_test_bored[i]:\n",
    "            same_wrong_2 += 1\n",
    "        if test[i] != X_test_bored[i]:\n",
    "            dif_wrong_2 += 1\n",
    "    \n",
    "    if test[i] == 3 and pred[i] == 3:\n",
    "        if test[i] == X_test_bored[i]:\n",
    "            same_correct_3 += 1\n",
    "        if test[i] != X_test_bored[i]:\n",
    "            dif_correct_3 += 1\n",
    "    if test[i] == 3 and pred[i] != 3:\n",
    "        if test[i] == X_test_bored[i]:\n",
    "            same_wrong_3 += 1\n",
    "        if test[i] != X_test_bored[i]:\n",
    "            dif_wrong_3 += 1\n",
    "        \n",
    "    if test[i] == 4 and pred[i] == 4:\n",
    "        if test[i] == X_test_bored[i]:\n",
    "            same_correct_4 += 1\n",
    "        if test[i] != X_test_bored[i]:\n",
    "            dif_correct_4 += 1\n",
    "    if test[i] == 4 and pred[i] != 4:\n",
    "        if test[i] == X_test_bored[i]:\n",
    "            same_wrong_4 += 1\n",
    "        if test[i] != X_test_bored[i]:\n",
    "            dif_wrong_4 += 1\n",
    "    \n",
    "    if test[i] == 5 and pred[i] == 5:\n",
    "        if test[i] == X_test_bored[i]:\n",
    "            same_correct_5 += 1\n",
    "        if test[i] != X_test_bored[i]:\n",
    "            dif_correct_5 += 1\n",
    "    if test[i] == 5 and pred[i] != 5:\n",
    "        if test[i] == X_test_bored[i]:\n",
    "            same_wrong_5 += 1\n",
    "        if test[i] != X_test_bored[i]:\n",
    "            dif_wrong_5 += 1\n",
    "\n",
    "print(\"Total length: \", len(X_test_bored))\n",
    "\n",
    "print(\"Same Correct 0 guesses is: \", same_correct_0)\n",
    "print(\"Same Wrong 0 guesses is: \", same_wrong_0)\n",
    "print(\"Dif Correct 0 guesses is: \", dif_correct_0)\n",
    "print(\"Dif Wrong 0 guesses is: \", dif_wrong_0)\n",
    "print('')\n",
    "print(\"Same Correct 1 guesses is: \", same_correct_1)\n",
    "print(\"Same Wrong 1 guesses is: \", same_wrong_1)\n",
    "print(\"Dif Correct 1 guesses is: \", dif_correct_1)\n",
    "print(\"Dif Wrong 1 guesses is: \", dif_wrong_1)\n",
    "print('')\n",
    "print(\"Same Correct 2 guesses is: \", same_correct_2)\n",
    "print(\"Same Wrong 2 guesses is: \", same_wrong_2)\n",
    "print(\"Dif Correct 2 guesses is: \", dif_correct_2)\n",
    "print(\"Dif Wrong 2 guesses is: \", dif_wrong_2)\n",
    "print('')\n",
    "print(\"Same Correct 3 guesses is: \", same_correct_3)\n",
    "print(\"Same Wrong 3 guesses is: \", same_wrong_3)\n",
    "print(\"Dif Correct 3 guesses is: \", dif_correct_3)\n",
    "print(\"Dif Wrong 3 guesses is: \", dif_wrong_3)\n",
    "print('')\n",
    "print(\"Same Correct 4 guesses is: \", same_correct_4)\n",
    "print(\"Same Wrong 4 guesses is: \", same_wrong_4)\n",
    "print(\"Dif Correct 4 guesses is: \", dif_correct_4)\n",
    "print(\"Dif Wrong 4 guesses is: \", dif_wrong_4)\n",
    "print('')\n",
    "print(\"Same Correct 5 guesses is: \", same_correct_5)\n",
    "print(\"Same Wrong 5 guesses is: \", same_wrong_5)\n",
    "print(\"Dif Correct 5 guesses is: \", dif_correct_5)\n",
    "print(\"Dif Wrong 5 guesses is: \", dif_wrong_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_0, mae_1, mae_2, mae_3, mae_4, mae_5 = (0,0,0,0,0,0)\n",
    "\n",
    "for i in range(len(pred)):\n",
    "    if test[i] == 0 and pred[i] != 0:\n",
    "        mae_0 += (abs(test[i]-pred[i]))\n",
    "    if test[i] == 1 and pred[i] != 1:\n",
    "        mae_1 += (abs(test[i]-pred[i]))\n",
    "    if test[i] == 2 and pred[i] != 2:\n",
    "        mae_2 += (abs(test[i]-pred[i]))\n",
    "    if test[i] == 3 and pred[i] != 3:\n",
    "        mae_3 += (abs(test[i]-pred[i]))\n",
    "    if test[i] == 4 and pred[i] != 4:\n",
    "        mae_4 += (abs(test[i]-pred[i]))\n",
    "    if test[i] == 5 and pred[i] != 5:\n",
    "        mae_5 += (abs(test[i]-pred[i]))\n",
    "\n",
    "cnt_0, cnt_1, cnt_2, cnt_3, cnt_4, cnt_5 = (0,0,0,0,0,0)\n",
    "for z in range(len(test)):\n",
    "    if test[z] == 0:\n",
    "        cnt_0 += 1\n",
    "    if test[z] == 1:\n",
    "        cnt_1 += 1\n",
    "    if test[z] == 2:\n",
    "        cnt_2 += 1\n",
    "    if test[z] == 3:\n",
    "        cnt_3 += 1\n",
    "    if test[z] == 4:\n",
    "        cnt_4 += 1\n",
    "    if test[z] == 5:\n",
    "        cnt_5 += 1\n",
    "\n",
    "mae_macroaverage = ((mae_0/cnt_0) + (mae_1/cnt_1) + (mae_2/cnt_2) + (mae_3/cnt_3) + (mae_4/cnt_4) + (mae_5/cnt_5)) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro-averaged Mean Absolute error is:  2.1174\n",
      "Macro-averaged F1-score is:  0.1573\n"
     ]
    }
   ],
   "source": [
    "print(\"Macro-averaged Mean Absolute error is: \", round(mae_macroaverage, 4))\n",
    "print(\"Macro-averaged F1-score is: \", round(f1_score(test, pred, average='weighted'), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
